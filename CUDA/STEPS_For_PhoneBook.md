
# CUDA Phonebook Search â€” Step-by-Step Meaning

## STEP 0 â€” Headers, constants, structures

**Purpose:** Prepare everything required before execution.

* Include **C++ + CUDA libraries** for file handling, memory, and GPU execution.
* Define `MAX_STR_LEN = 50` â†’ fixed memory size per contact name (important for GPU).
* Create `ResultContact` struct â†’ used **after GPU search** to store and **sort matches alphabetically on CPU**.

ðŸ‘‰ GPU does searching, **CPU handles sorting & printing**.

---

## STEP 1 â€” Read command-line input

**Purpose:** Get user search parameters.

* User provides:

  * `search_string`
  * `threads_per_block`
* Replace `_` with space â†’ allows searching names like `"Md_Rahim"`.

ðŸ‘‰ Defines **what to search** and **how many GPU threads per block**.

---

## STEP 2 â€” Load phonebook on CPU

**Purpose:** Read file into normal RAM first.

* Open phonebook text file.
* Extract:

  * **name**
  * **phone number**
* Store in:

  * `host_names_vec`
  * `host_numbers_vec`
* Count total contacts.

ðŸ‘‰ GPU **cannot read files directly**, so CPU must load first.

---

## STEP 3 â€” Prepare flat memory for GPU

**Purpose:** Convert flexible C++ strings â†’ fixed GPU-friendly array.

* Allocate:

  * `h_names` â†’ continuous char array (`num_contacts Ã— MAX_STR_LEN`)
  * `h_results` â†’ match flags.
* Copy each name into fixed-size slot.

ðŸ‘‰ GPUs work best with **simple linear memory**, not `vector<string>`.

---

## STEP 4 â€” Allocate GPU memory

**Purpose:** Create storage on the **device (VRAM)**.

* `d_names` â†’ all names
* `d_results` â†’ match results
* `d_search_name` â†’ search keyword

ðŸ‘‰ Without this, GPU **cannot access data**.

---

## STEP 5 â€” Copy CPU â†’ GPU

**Purpose:** Move data from **host RAM â†’ device VRAM**.

* Transfer:

  * names
  * search string

ðŸ‘‰ This transfer is **mandatory before kernel execution**.

---

## STEP 6 â€” Configure CUDA grid

**Purpose:** Decide **parallel execution size**.

* Compute number of **blocks** needed:

```
blocks = ceil(num_contacts / threads_per_block)
```

ðŸ‘‰ Ensures **one GPU thread per contact**.

---

## STEP 7 â€” Launch kernel (parallel search)

**Purpose:** Perform **actual phonebook search on GPU**.

Inside kernel:

1. Each thread computes **global index**

```
idx = blockIdx.x * blockDim.x + threadIdx.x
```

2. If index valid:

   * Read **one contact name**
   * Run **substring match (`check`)**
   * Store **0 or 1** in results.

ðŸ‘‰ **Thousands of contacts searched simultaneously**.

---

## STEP 8 â€” Copy results back to CPU

**Purpose:** Retrieve GPU computation outcome.

* Transfer `d_results â†’ h_results`.

ðŸ‘‰ CPU now knows **which contacts matched**.

---

## STEP 9 â€” Collect matched contacts

**Purpose:** Build readable result list.

* For each index:

  * If result = 1 â†’ push `{name, number}` into vector.

ðŸ‘‰ Converts **binary flags â†’ real contacts**.

---

## STEP 10 â€” Sort alphabetically

**Purpose:** Improve output readability.

* Use C++ `sort()` with overloaded `<` operator.

ðŸ‘‰ Sorting is done on **CPU (simpler than GPU sorting)**.

---

## STEP 11 â€” Print results

**Purpose:** Show final search output.

* Display:

```
Name  Number
```

in **ascending order**.

---

## STEP 12 â€” Free memory

**Purpose:** Prevent memory leaks.

* Free:

  * CPU memory (`free`)
  * GPU memory (`cudaFree`)

ðŸ‘‰ Always required in CUDA programs.

---

# One-Line Flow Summary

```
Read file (CPU)
   â†“
Copy data to GPU
   â†“
Parallel search by thousands of threads
   â†“
Copy results back
   â†“
Sort + print on CPU
```
